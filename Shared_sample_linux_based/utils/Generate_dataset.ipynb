{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Disabled too many blank lines in python --> settings --> inspection --> python --> coding stlye PEP8 --> added code E303\n",
    "\n",
    "import numpy as np          # dealing with arrays\n",
    "import os                   # dealing with directories\n",
    "import pandas as pd\n",
    "from tqdm import tqdm       # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk\n",
    "data_files_dir = '/home/moe/PycharmProjects/clean_code/Cleaned_code/data_files/'\n",
    "testing_dir = '/home/moe/Desktop/bambo_Logs/Data/my_dataset/testing/'\n",
    "validation_dir = '/home/moe/Desktop/bambo_Logs/Data/my_dataset/validation/'\n",
    "training_dir = '/home/moe/Desktop/bambo_Logs/Data/my_dataset/training/'\n",
    "\n",
    "\n",
    "HOME_DIR = '/home/moe/PycharmProjects/clean_code/'\n",
    "elevator_path = '/home/moe/Desktop/bambo_Logs/Data/elevator_fulldata/process/'\n",
    "excel_file = '/home/moe/Desktop/bambo_Logs/Data/weight_log_summary_602020.xlsx'\n",
    "dataset_path = '/home/moe/Desktop/bambo_Logs/Data/my_dataset/'\n",
    "\n",
    "excel_data = []\n",
    "dfs_train = pd.read_excel(excel_file, sheet_name='All_training')\n",
    "dfs_test = pd.read_excel(excel_file, sheet_name='All_testing')\n",
    "dfs_validate = pd.read_excel(excel_file, sheet_name='All_validation')\n",
    "excel_data.append(dfs_train.get_values())\n",
    "excel_data.append(dfs_test.get_values())\n",
    "excel_data.append(dfs_validate.get_values())\n",
    "# print(len(excel_data[0]), len(excel_data[1]), len(excel_data[2]))\n",
    "\n",
    "# excel_data[dataset][element_index][0 = weigh and 1= log_name]\n",
    "\n",
    "elev_files_dir = sorted(os.listdir(elevator_path))\n",
    "subsets_dir = sorted(os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_inner = []\n",
    "volume_inner = []\n",
    "speed_complete = []\n",
    "elevator_files = []\n",
    "split = []\n",
    "weight_arr = []\n",
    "log_path_arr = []\n",
    "img_path_arr = []\n",
    "log_name_arr = []\n",
    "speed_arr = []\n",
    "volume_arr = []\n",
    "full_data_dict = {}\n",
    "full_data = []\n",
    "log_length = []\n",
    "missing_speed = []\n",
    "missing_volume = []\n",
    "\n",
    "random = False\n",
    "\n",
    "for n, file in enumerate(elev_files_dir):\n",
    "    file_ = file.split('.')[0]\n",
    "    splity = file_.split('_')\n",
    "    file_temp = ''\n",
    "    for m in range(len(splity)):\n",
    "        if m != 3:\n",
    "            file_temp += splity[m]+'_'\n",
    "    file_temp = file_temp.rstrip('_')\n",
    "    elevator_files.append(file_temp)\n",
    "log_ = ''\n",
    "\n",
    "for i, subset in enumerate(subsets_dir):\n",
    "    full_data_dict.clear()\n",
    "    speed_arr.clear()\n",
    "    volume_arr.clear()\n",
    "    img_path_arr.clear()\n",
    "    weight_arr.clear()\n",
    "    log_path_arr.clear()\n",
    "    log_name_arr.clear()\n",
    "    log_length.clear()\n",
    "    subset_name = subset\n",
    "    subset_path = dataset_path + subset + '/'\n",
    "    subset_dir = sorted(os.listdir(subset_path))\n",
    "    if random:\n",
    "        np.random.shuffle(subset_dir)\n",
    "    # print(subset_dir)\n",
    "    for j, log in enumerate(subset_dir):\n",
    "\n",
    "        log_path = subset_path + log + '/'\n",
    "        log_dir = sorted(os.listdir(log_path))\n",
    "        log_temp = ''\n",
    "        splity = log.split('_')\n",
    "        weight = np.float32(splity[len(splity)-1])\n",
    "        for m in range(len(splity)-1):\n",
    "            if m != 3:\n",
    "                log_temp += splity[m]+'_'\n",
    "            log_ = log_temp.rstrip('_')\n",
    "        for n, file in enumerate(elev_files_dir):\n",
    "            file_ = elevator_files[n]\n",
    "            file_path = elevator_path + file\n",
    "            if file_ == log_:\n",
    "                speed_inner.clear()\n",
    "                volume_inner.clear()\n",
    "                missing_speed.clear()\n",
    "                missing_volume.clear()\n",
    "                \n",
    "                log_file = open(file_path, \"r\")\n",
    "                for line in log_file:\n",
    "                    split = line.split(\" \")\n",
    "                    speed = np.float32(split[6])\n",
    "                    volume = np.float32(split[10])\n",
    "                    speed_inner.append(speed)\n",
    "                    volume_inner.append(volume)\n",
    "                gap = len(log_dir) - len(speed_inner)\n",
    "                for element in range(gap):\n",
    "                    missing_speed.append(np.float32(0.0))\n",
    "                    missing_volume.append(np.float32(0.0))\n",
    "                speed_arr = speed_arr + missing_speed + speed_inner\n",
    "                volume_arr = volume_arr + missing_volume + volume_inner\n",
    "                for img in log_dir:\n",
    "                    img_path = log_path + img\n",
    "                    img_path_arr.append(img_path)\n",
    "                    log_name_arr.append(log)\n",
    "                    log_path_arr.append(log_path)\n",
    "                    weight_arr.append(weight)\n",
    "                    log_length.append(len(log_dir))\n",
    "\n",
    "                break\n",
    "    #    \"\"\"\n",
    "    #    This code saves data to a numpy array instead of pickle\n",
    "    #\n",
    "    # weight_arr_np = np.array(weight_arr)\n",
    "    # log_name_arr_np = np.array(log_name_arr)\n",
    "    # log_path_arr_np = np.array(log_path_arr)\n",
    "    # speed_arr_np = np.array(speed_arr)\n",
    "    # full_data_dict = {'speed': speed_arr_np, 'log_path': log_path_arr_np, 'log_name': log_name_arr_np, 'weight': weight_arr_np}\n",
    "    # np.save(subset_name+ '_data.npy', full_data_dict)\n",
    "    #\n",
    "    #     \"\"\"\n",
    "    #     break   # This break to generate a single log\n",
    "    if random:\n",
    "        randomized = \"_randomized_\"\n",
    "    else: randomized = \"_\"\n",
    "    full_data_dict = {\"volume\": volume_arr, \"speed\": speed_arr, \"log_path\": log_path_arr, \"img_path\": img_path_arr, \"log_name\": log_name_arr,\"log_length\": \n",
    "        log_length, \"weight\": weight_arr}\n",
    "    pickle_out = open(data_files_dir+subset_name+randomized+\"dataset.pickle\", \"wb\")\n",
    "    pk.dump(full_data_dict, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /home/moe/Desktop/bambo_Logs/Data/my_dataset/training/2015_07_20_1746_bamb_41_609/609_0_3609.694-0.739352.bmp   609.0   0.73935   423 0.01221 19682 \n",
      "\n",
      "\n",
      "70002 21297 26496\n"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "import random\n",
    "\n",
    "HOME_DIR = '/home/moe/PycharmProjects/clean_code/'\n",
    "MASK_path = '/home/moe/Desktop/bambo_Logs/mask/mask_binary.bmp'\n",
    "\n",
    "#  data shuffling is done through this, so there is no need to shuffle it in tensorflow\n",
    "randomize = False\n",
    "if randomize:\n",
    "    randomized = \"_randomized_\"\n",
    "else: \n",
    "    randomized = \"_\"\n",
    "    \n",
    "pickle_in_train = open(data_files_dir + \"training\"+randomized+\"dataset.pickle\", \"rb\")\n",
    "training_data = pk.load(pickle_in_train)\n",
    "pickle_in_train.close()\n",
    "\n",
    "\n",
    "pickle_in_validate = open(data_files_dir + \"validation\"+randomized+\"dataset.pickle\", \"rb\")\n",
    "validation_data = pk.load(pickle_in_validate)\n",
    "pickle_in_validate.close()\n",
    "pickle_in_test = open(data_files_dir + \"testing\"+randomized+\"dataset.pickle\", \"rb\")\n",
    "testing_data = pk.load(pickle_in_test)\n",
    "pickle_in_test.close()\n",
    "\n",
    "\n",
    "training_weight = training_data['weight']\n",
    "training_path = training_data['img_path']\n",
    "training_speed = training_data['speed']\n",
    "training_log_length = training_data['log_length']\n",
    "training_volume = training_data['volume']\n",
    "\n",
    "\n",
    "# img = plt.imread(training_path[0])\n",
    "\n",
    "\n",
    "validation_weight = validation_data['weight']\n",
    "validation_path = validation_data['img_path']\n",
    "validation_speed = validation_data['speed']\n",
    "validation_log_length = validation_data['log_length']\n",
    "validation_volume = validation_data['volume']\n",
    "\n",
    "testing_weight = testing_data['weight']\n",
    "testing_path = testing_data['img_path']\n",
    "testing_speed = testing_data['speed']\n",
    "testing_log_length = testing_data['log_length']\n",
    "testing_volume = testing_data['volume']\n",
    "\n",
    "index = random.randint(1, len(validation_log_length))\n",
    "# index = 1000\n",
    "\n",
    "print(' ', training_path[index], ' ', training_weight[index], ' ', training_speed[index], ' ', training_log_length[index],  training_volume[index], \n",
    "      index,\n",
    "      '\\n\\n')\n",
    "\n",
    "print(len(training_log_length), len(testing_log_length), len(validation_log_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
