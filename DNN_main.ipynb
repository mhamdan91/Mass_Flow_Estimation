{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "from termcolor import colored\n",
    "import tensorflow as tf\n",
    "import os                   # work with directories\n",
    "import numpy as np          # dealing with arrays\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "import pickle as pk\n",
    "import sys\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.python.eager import tape\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "layers = tf.keras.layers\n",
    "tf.enable_eager_execution(config=config)\n",
    "tf.executing_eagerly()\n",
    "print(tf.__version__)\n",
    "\n",
    "HOME_DIR = os.getcwd()\n",
    "MAIN_dir = HOME_DIR + '/Cleaned_code/Shared_git/'\n",
    "checkpoint_path = MAIN_dir + 'checkpoints/'\n",
    "MASK_path = MAIN_dir + 'mask/mask_binary_mod.bmp'\n",
    "data_files_path = MAIN_dir + 'data_files/'\n",
    "mean_path = data_files_path + 'std_mean_60_dataset.npy'\n",
    "dataset_path = MAIN_dir + 'dataset/'\n",
    "tensorboard_path = MAIN_dir + 'tensorboard/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD DATASET\n",
    "\n",
    "pickle_in_train = open(MAIN_dir + 'data_files/' + \"training_dataset.pickle\", \"rb\")\n",
    "training_data = pk.load(pickle_in_train)\n",
    "pickle_in_train.close()\n",
    "\n",
    "pickle_in_validate = open(MAIN_dir + 'data_files/' + \"validation_dataset.pickle\", \"rb\")\n",
    "validation_data = pk.load(pickle_in_validate)\n",
    "pickle_in_validate.close()\n",
    "pickle_in_test = open(MAIN_dir + 'data_files/' + \"testing_dataset.pickle\", \"rb\")\n",
    "testing_data = pk.load(pickle_in_test)\n",
    "pickle_in_test.close()\n",
    "\n",
    "training_path = training_data['img_path']\n",
    "training_weight = np.float32(np.reshape(training_data['weight'], [-1, 1]))\n",
    "training_speed = np.reshape(training_data['speed'], [-1, 1])\n",
    "training_volume = np.reshape(training_data['volume'], [-1, 1])\n",
    "training_log_length = np.float32(training_data['log_length'])\n",
    "\n",
    "validation_path = validation_data['img_path']\n",
    "validation_weight = np.float32(np.reshape(validation_data['weight'], [-1, 1]))\n",
    "validation_speed = np.reshape(validation_data['speed'], [-1, 1])\n",
    "validation_volume = np.reshape(validation_data['volume'], [-1, 1])\n",
    "validation_log_length = np.float32(validation_data['log_length'])\n",
    "\n",
    "testing_path = testing_data['img_path']\n",
    "testing_weight = np.float32(np.reshape(testing_data['weight'], [-1, 1]))\n",
    "testing_speed = np.reshape(testing_data['speed'], [-1, 1])\n",
    "testing_volume = np.reshape(testing_data['volume'], [-1, 1])\n",
    "testing_log_length = np.float32(testing_data['log_length'])\n",
    "\n",
    "std_mean_data = np.load(mean_path)\n",
    "mean_std = np.array(std_mean_data, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_function(filename, weight, speed, log_length):\n",
    "    # Get the image as raw bytes.\n",
    "    image_name = tf.read_file(filename)\n",
    "    # Decode the raw bytes so it becomes a tensor with type.\n",
    "    image = tf.image.decode_bmp(image_name, channels=3)\n",
    "    \n",
    "    # The type is now uint8 but we need it to be float.\n",
    "    image = tf.to_float(image)\n",
    "\n",
    "    return image, weight, speed, log_length\n",
    "\n",
    "\n",
    "def data_normalization(image, weight, speed, log_length):\n",
    "    mean_std = np.load(mean_path)  \n",
    "    mean = [mean_std[0], mean_std[2], mean_std[4]]\n",
    "    std_dev = [mean_std[1], mean_std[3], mean_std[5]]\n",
    "    mean = tf.reshape(mean, [1, 1, 3])\n",
    "    std_dev = tf.reshape(std_dev, [1, 1, 3])\n",
    "    img_m = image - mean\n",
    "    image = img_m / std_dev\n",
    "  \n",
    "    return image, weight, speed, log_length\n",
    "\n",
    "\n",
    "\n",
    "def data_resize(image, weight, speed, log_length):\n",
    "    resized_image = tf.image.resize_images(image, size=(96, 144))\n",
    "  \n",
    "    return resized_image, weight, speed, log_length\n",
    "\n",
    "\n",
    "def data_masking(image, weight, speed, log_length):\n",
    "    mask_name = tf.read_file(MASK_path)\n",
    "    mask = tf.image.decode_bmp(mask_name, channels=3)\n",
    "    mask = tf.to_float(mask)\n",
    "    mask = tf.image.resize_images(mask, size=(96, 144))\n",
    "    masked_img = tf.multiply(image, mask)\n",
    "    \n",
    "    return masked_img, weight, speed, log_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(prediction, label, train_log_length_, operation='L2'):\n",
    "    if operation == 'L2':\n",
    "        return tf.divide(tf.squared_difference(prediction, label), train_log_length_)\n",
    "    elif operation == 'Subtraction':\n",
    "        return tf.divide(tf.subtract(prediction, label), train_log_length_)\n",
    "    elif operation == 'L1':\n",
    "        return tf.divide(tf.abs(tf.subtract(prediction, label)), train_log_length_)\n",
    "    else:\n",
    "        raise ValueError('Please specify loss function (L2, L1, Subtraction)')\n",
    "\n",
    "\n",
    "def print_progress(count, total, cnt, overall, time_, count_log, loss, loss_):\n",
    "    percent_complete = float(count) / total\n",
    "    overall_complete = float(cnt) / (overall-1)\n",
    "\n",
    "    sec = time_ % 60\n",
    "    mint = int(time_/60) % 60\n",
    "    hr = int(time_/3600) % 60\n",
    "    loss = str(loss)\n",
    "    loss_ = str(loss_)\n",
    "    msg = \"\\r Time_lapsed (hr:mm:ss) --> {0:02d}:{1:02d}:{2:02d} ,   loss: {3:s}   Log Progress: {4:.1%},     Overall Progress:{5:.1%},\" \\\n",
    "        \" completed {6:d} out of 185 logs <--> Initial loss: {7:s} \".format(hr, mint, sec, loss, percent_complete, overall_complete, count_log, loss_)\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "\n",
    "def validation_progress(log_cnt, num_logs, time_, loss, accuracy_loc):\n",
    "    log_cnt += 1\n",
    "    overall_complete = float(log_cnt) / num_logs\n",
    "    sec = int(time_) % 60\n",
    "    mint = int(time_/60) % 60\n",
    "    hr = int(time_/3600) % 60\n",
    "    loss = str(loss)\n",
    "    msg = \"\\r Validation_Time (hr:mm:ss) --> {0:02d}:{1:02d}:{2:02d} ,   Avg_loss: {3:s}   Avg_accuracy: {4:.1%}   Overall Progress:{5:.1%},\" \\\n",
    "        \" completed {6:d} out of {7:d} logs\".format(hr, mint, sec, loss, accuracy_loc, overall_complete, log_cnt, num_logs)\n",
    "    sys.stdout.write(colored(msg, 'green'))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def write_summaries(loss, i, global_step, vars_loc, grads_loc, train=True):\n",
    "    with summary_writer.as_default():\n",
    "        with tf.contrib.summary.always_record_summaries():\n",
    "            if train:\n",
    "                tf.contrib.summary.scalar(\"train_loss\", loss, step=global_step)\n",
    "                tf.contrib.summary.scalar(\"step\", i, step=global_step)\n",
    "                #  do not add spaces after names\n",
    "                tf.contrib.summary.histogram(\"weights\", vars_loc, step=global_step)\n",
    "                tf.contrib.summary.histogram(\"gradients\", grads_loc, step=global_step)\n",
    "            else:\n",
    "                tf.contrib.summary.scalar(\"val_loss\", loss, step=global_step)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Batch_size = 64\n",
    "Buffer_size = 64\n",
    "Epochs = 100\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((training_path, training_weight, training_speed, training_log_length))\n",
    "train_dataset = train_dataset.map(parse_function, num_parallel_calls=8)\n",
    "train_dataset = train_dataset.map(data_resize, num_parallel_calls=8)\n",
    "train_dataset = train_dataset.map(data_normalization, num_parallel_calls=8)\n",
    "train_dataset = train_dataset.map(data_masking, num_parallel_calls=8)\n",
    "train_dataset = train_dataset.batch(Batch_size)\n",
    "train_dataset = train_dataset.prefetch(Batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testing_path, testing_weight, testing_speed, testing_log_length))\n",
    "test_dataset = test_dataset.map(parse_function, num_parallel_calls=8)\n",
    "test_dataset = test_dataset.map(data_resize, num_parallel_calls=8)\n",
    "test_dataset = test_dataset.map(data_normalization, num_parallel_calls=8)\n",
    "test_dataset = test_dataset.map(data_masking, num_parallel_calls=8)\n",
    "test_dataset = test_dataset.batch(Batch_size)\n",
    "test_dataset = test_dataset.prefetch(Batch_size)\n",
    "\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_path, validation_weight, validation_speed, validation_log_length))\n",
    "validation_dataset = validation_dataset.map(parse_function, num_parallel_calls=8)\n",
    "validation_dataset = validation_dataset.map(data_resize, num_parallel_calls=8)\n",
    "validation_dataset = validation_dataset.map(data_normalization, num_parallel_calls=8)\n",
    "validation_dataset = validation_dataset.map(data_masking, num_parallel_calls=8)\n",
    "validation_dataset = validation_dataset.batch(Batch_size)\n",
    "validation_dataset = validation_dataset.prefetch(Batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD NETWORK\n",
    "network_size = None\n",
    "if network_size == 9:\n",
    "    # LOAD RES-9E\n",
    "    from Cleaned_code.Shared_git import RES_9E as resnet50\n",
    "elif network_size == 16:\n",
    "    # LOAD RES-16E\n",
    "    from Cleaned_code.Shared_git import RES_16E as resnet50\n",
    "else:\n",
    "    # LOAD RES-9ER - default\n",
    "    from Cleaned_code.Shared_git import RES_9ER as resnet50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and configure tensorbaord and checkpoints\n",
    "data_format = 'channels_last'\n",
    "model = resnet50.ResNet50(data_format=data_format, include_top=True, pooling=None, classes=1)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "accuracy = 0 \n",
    "acc_prediction = tf.constant(0, dtype=\"float32\")\n",
    "t = tf.constant(1/7.5, dtype=\"float32\")\n",
    "loss = tf.constant(0, dtype=\"float32\")\n",
    "logdir = tensorboard_path\n",
    "checkpont_path = checkpoint_path+\"cp-{log:06d}.ckpt\"\n",
    "checkpont_dir = os.path.dirname(checkpoint_path)\n",
    "summary_writer = tf.contrib.summary.create_file_writer(logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Use trained model to test results\n",
    "checkpoint_name = 'RES_9ER'\n",
    "model.load_weights(checkpoint_path+checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION CODE\n",
    "\n",
    "def validate_model(model_loc, val_step_, n_logs, data_set, write_summary=True, return_losses=True):\n",
    "    logs_N = n_logs\n",
    "    start = time.time()\n",
    "    MAE = []\n",
    "    MSE = []\n",
    "    MSBE = []\n",
    "    log_order = []\n",
    "    Accuracy = []\n",
    "    metric_mse = []\n",
    "    summed_volume = 0\n",
    "    remainder = 0\n",
    "    iterations = 0\n",
    "    fixed_size = 0\n",
    "    count = 0\n",
    "    new_log = True\n",
    "    cnt_log = -1\n",
    "    signal = []\n",
    "    signals = []\n",
    "    for (batch, (images, labels, speeds, log_length)) in enumerate(data_set):\n",
    "        batch += 1\n",
    "        if new_log:           \n",
    "            new_log = False\n",
    "            length = int(log_length[0].numpy() - fixed_size + remainder)\n",
    "            iterations = int(np.floor(length/Batch_size))\n",
    "            remainder = int(np.mod(length, Batch_size))\n",
    "            fixed_size = np.shape(speeds)[0]\n",
    "            cnt_log += 1\n",
    "\n",
    "        if count < iterations:\n",
    "            mass_pred = model_loc(images)\n",
    "            volume = (mass_pred * speeds) * t\n",
    "            summed_volume += tf.reduce_sum(volume)\n",
    "            mass = np.abs(summed_volume.numpy())\n",
    "            count = count + 1\n",
    "            signal.append(volume)\n",
    "            # Compute loss\n",
    "            if count == iterations and remainder == 0:\n",
    "                loss_unsigned = np.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='L2').numpy())\n",
    "                loss_outlier = np.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='L1').numpy())\n",
    "                loss_signed = np.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='Subtraction').numpy())\n",
    "                MSE.append(loss_unsigned)\n",
    "                MAE.append(loss_outlier)\n",
    "                MSBE.append(loss_signed)\n",
    "                signals.append(signal[0:len(signal)])\n",
    "                signal.clear()\n",
    "        \n",
    "                gt = np.squeeze(labels[0].numpy())\n",
    "                metric_mse.append(loss_unsigned)\n",
    "                \n",
    "                tmp = np.abs(gt - mass)\n",
    "                tmp = tmp / gt\n",
    "                log_acc = 1 - np.squeeze(tmp)\n",
    "                    \n",
    "                Accuracy.append(log_acc)\n",
    "                log_order.append(np.squeeze(log_length[0].numpy()))\n",
    "                time_ = time.time() - start\n",
    "                validation_progress(cnt_log, logs_N, time_, np.mean(metric_mse), np.mean(Accuracy))\n",
    "                if write_summary:\n",
    "                    write_summaries(np.mean(MSE), 0, val_step_, 0, 0, train=False)\n",
    "                val_step_ += 1\n",
    "                # Reset\n",
    "                summed_volume = 0\n",
    "                count = 0\n",
    "                fixed_size = 0\n",
    "                new_log = True\n",
    "        # Handle the remainder from iterations\n",
    "        else:\n",
    "            # Compute and Aggregate the remainder Gradients\n",
    "            mass_pred = model(images[0:remainder])\n",
    "            volume = (mass_pred * speeds[0:remainder]) * t\n",
    "            summed_volume += tf.reduce_sum(volume)\n",
    "            mass = np.abs(summed_volume.numpy())\n",
    "            signal.append(volume)\n",
    "            \n",
    "            loss_unsigned = np.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='L2').numpy())\n",
    "            loss_outlier = np.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='L1').numpy())\n",
    "            loss_signed = np.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='Subtraction').numpy())\n",
    "            MSE.append(loss_unsigned)\n",
    "            MAE.append(loss_outlier)\n",
    "            MSBE.append(loss_signed)\n",
    "            signals.append(signal[0:len(signal)])\n",
    "            signal.clear()\n",
    "            \n",
    "            gt = np.squeeze(labels[0].numpy())\n",
    "            metric_mse.append(loss_unsigned)\n",
    "\n",
    "            tmp = np.abs(gt - mass)\n",
    "            tmp = tmp / gt\n",
    "            log_acc = 1 - np.squeeze(tmp)\n",
    "            \n",
    "            Accuracy.append(log_acc)\n",
    "            log_order.append(np.squeeze(log_length[0].numpy()))\n",
    "            time_ = time.time() - start\n",
    "            validation_progress(cnt_log, logs_N, time_, np.mean(metric_mse), np.mean(Accuracy))  \n",
    "            \n",
    "            if write_summary:  \n",
    "                write_summaries(np.mean(MSE), 0, val_step_, 0, 0, train=False)\n",
    "            val_step_ += 1\n",
    "            summed_volume = 0\n",
    "            count = 0\n",
    "            new_log = True\n",
    "            \n",
    "            # Handle gradients for the next log\n",
    "            if cnt_log != logs_N-1:    \n",
    "                mass_pred = model(images[remainder:])\n",
    "                volume = (mass_pred * speeds[remainder:]) * t\n",
    "                summed_volume += tf.reduce_sum(volume)\n",
    "                signal.append(volume)\n",
    "                \n",
    "    if return_losses:\n",
    "        return MSE, MAE, MSBE, Accuracy, val_step_\n",
    "    else: \n",
    "        return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING CODE\n",
    "save_epoch = 1\n",
    "init_vol = 0\n",
    "summed_vol_diff = 0\n",
    "volume_diff = []\n",
    "aggregated_diff = []\n",
    "lamda = 0.05\n",
    "loss_ = 0\n",
    "logs_N = 1\n",
    "start = time.time()\n",
    "aggregated = []\n",
    "MSE_log = []\n",
    "MAE_log = []\n",
    "MSBE_log = []\n",
    "MSE_t = []\n",
    "MAE_t = []\n",
    "MSBE_t = []\n",
    "MSE_avg = []\n",
    "MAE_avg = []\n",
    "MSBE_avg = []\n",
    "MSE_train = []\n",
    "val_step = 1\n",
    "run_name = 'random'\n",
    "performance_flag = True\n",
    "for epoch in range(Epochs):\n",
    "    loss_metric = 0\n",
    "    summed_volume = 0\n",
    "    remainder = 0\n",
    "    iterations = 0\n",
    "    fixed_size = 0\n",
    "    count = 0\n",
    "    new_log = True\n",
    "    append_flag = True\n",
    "    next_log = False\n",
    "    cnt_log = -1\n",
    "    first_run = True\n",
    "    for (batch, (images, labels, speeds, log_length)) in enumerate(train_dataset):\n",
    "        batch += 1\n",
    "        step = tf.train.get_or_create_global_step()\n",
    "        if new_log:           \n",
    "            new_log = False\n",
    "            length = int(log_length[0].numpy() - fixed_size + remainder)\n",
    "            iterations = int(np.floor(length/Batch_size))\n",
    "            remainder = int(np.mod(length, Batch_size))\n",
    "            fixed_size = np.shape(speeds)[0]\n",
    "            cnt_log += 1\n",
    "\n",
    "        if count < iterations:\n",
    "\n",
    "            # Compute and Aggregate Gradients\n",
    "            volume_diff.clear()\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                mass_pred = model(images)   \n",
    "                # print(np.shape(mass_pred))\n",
    "                volume = (mass_pred * speeds) * t\n",
    "                # print(volume)\n",
    "                for ixd, vol in enumerate(volume):\n",
    "                    if ixd == 0:\n",
    "                        summed_vol_diff += tf.squared_difference(volume[ixd], init_vol)\n",
    "                        volume_diff.append(tf.squared_difference(volume[ixd], init_vol))\n",
    "                    else:\n",
    "                        summed_vol_diff += tf.squared_difference(volume[ixd], volume[ixd-1])\n",
    "                        volume_diff.append(tf.squared_difference(volume[ixd], volume[ixd-1]))\n",
    "                init_vol = volume[Batch_size-1]\n",
    "                summed_volume += tf.reduce_sum(volume)\n",
    "                watched_vars = tape.watched_variables()\n",
    "            grads = tape.gradient(volume, model.trainable_variables)\n",
    "            grads_diff = tape.gradient(volume_diff, model.trainable_variables)\n",
    "            del tape\n",
    "\n",
    "            if count == 0 and append_flag:\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    aggregated.append(grad)\n",
    "                    aggregated_diff.append(grads_diff[idx])\n",
    "            else:\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    aggregated[idx] = grad+aggregated[idx]\n",
    "                    aggregated_diff[idx] = grads_diff[idx] + aggregated_diff[idx] \n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "            # Compute loss and Apply Gradients\n",
    "            if count == iterations and remainder == 0:\n",
    "                loss = compute_loss(summed_volume, labels[0], log_length[0], operation='Subtraction')\n",
    "                loss_metric = tf.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='L2').numpy())\n",
    "                for idx, grd in enumerate(aggregated):\n",
    "                    aggregated[idx] = (loss * 2 * grd) + (summed_vol_diff * 2 * lamda * aggregated_diff[idx] / log_length[0])\n",
    "                rmse = tf.sqrt(tf.abs(loss_metric))\n",
    "                optimizer.apply_gradients(zip(aggregated, model.trainable_variables), global_step=step)\n",
    "                var_list = model.variables\n",
    "                write_summaries(loss_metric, batch, step, var_list[0], grads[0], train=True)\n",
    "                end = time.time() - start\n",
    "                sec = int(end % 60)\n",
    "                mint = int(end/60) % 60\n",
    "                hr = int(end/3600) % 60\n",
    "                # print(volume[0])\n",
    "                print(\"\\r Time_lapsed (hr:mm:ss) --> {:02d}:{:02d}:{:02d} Epoch: {}, Log: {}, Log_progress: {:.1%} - Overall_progress: {:.1%}, Length: {} \"\n",
    "                \"Label:{}, loss: {:.3f}, train RMSE: {:.2f}\".format(hr, mint, sec, epoch+1, cnt_log+1, (cnt_log+1)/logs_N, (epoch+1)/Epochs,  \n",
    "                log_length[0].numpy(), labels[0].numpy(), loss_metric, rmse))\n",
    "\n",
    "                if first_run:\n",
    "                    first_run = False\n",
    "                    loss_ = loss_metric.numpy()\n",
    "\n",
    "                # Reset\n",
    "                aggregated.clear()\n",
    "                aggregated_diff.clear()\n",
    "                summed_vol_diff = 0\n",
    "                init_vol = 0\n",
    "                summed_volume = 0\n",
    "                count = 0\n",
    "                append_flag = True\n",
    "                fixed_size = 0\n",
    "                new_log = True\n",
    "\n",
    "        # Handle the remainder from iterations\n",
    "        else:\n",
    "            # Compute and Aggregate the remainder Gradients\n",
    "            volume_diff.clear()\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                mass_pred = model(images[0:remainder])  \n",
    "                # print(np.shape(mass_pred))\n",
    "                volume = (mass_pred * speeds[0:remainder]) * t\n",
    "                for ixd, vol in enumerate(volume):\n",
    "                    if ixd == 0:\n",
    "                        summed_vol_diff += tf.squared_difference(volume[ixd], init_vol)\n",
    "                        volume_diff.append(tf.squared_difference(volume[ixd], init_vol))\n",
    "                    else:\n",
    "                        summed_vol_diff += tf.squared_difference(volume[ixd], volume[ixd-1])\n",
    "                        volume_diff.append(tf.squared_difference(volume[ixd], volume[ixd-1]))\n",
    "                init_vol = volume[remainder-1]\n",
    "                summed_volume += tf.reduce_sum(volume)\n",
    "                watched_vars = tape.watched_variables()\n",
    "            grads = tape.gradient(volume, model.trainable_variables)\n",
    "            grads_diff = tape.gradient(volume_diff, model.trainable_variables)\n",
    "            del tape\n",
    "\n",
    "            for idx, grad in enumerate(grads):\n",
    "                aggregated[idx] = grad+aggregated[idx]\n",
    "                aggregated_diff[idx] = grads_diff[idx] + aggregated_diff[idx] \n",
    "\n",
    "            # Compute loss and apply gradients for the remainder\n",
    "            loss = compute_loss(summed_volume, labels[0], log_length[0], operation='Subtraction')\n",
    "            loss_metric = tf.squeeze(compute_loss(summed_volume, labels[0], log_length[0], operation='L2').numpy())\n",
    "            for idx, grd in enumerate(aggregated):\n",
    "                aggregated[idx] = (loss * 2 * grd) + (summed_vol_diff * 2 * lamda * aggregated_diff[idx] / log_length[0])\n",
    "            rmse = tf.squeeze(tf.sqrt(tf.abs(loss_metric)))\n",
    "            optimizer.apply_gradients(zip(aggregated, model.trainable_variables), global_step=step)\n",
    "            var_list = model.trainable_variables\n",
    "            write_summaries(loss_metric, batch, step, var_list[0], grads[0], train=True)\n",
    "            end = time.time() - start\n",
    "            sec = int(end % 60)\n",
    "            mint = int(end/60) % 60\n",
    "            hr = int(end/3600) % 60\n",
    "            # print(volume[0])\n",
    "            print(\"\\r Time_lapsed (hr:mm:ss) --> {:02d}:{:02d}:{:02d} Epoch: {}, Log: {}, Log_progress: {:.1%} - Overall_progress: {:.1%}, Length: {} \"\n",
    "                  \"Label:{}, loss: {:.3f}, train RMSE: {:.2f}\".format(hr, mint, sec, epoch+1, cnt_log+1, (cnt_log+1)/logs_N, (epoch+1)/Epochs,  \n",
    "                  log_length[0].numpy(), labels[0].numpy(), loss_metric, rmse))\n",
    "\n",
    "            if first_run:\n",
    "                first_run = False\n",
    "                loss_ = loss_metric.numpy()\n",
    "\n",
    "            # Reset\n",
    "            aggregated.clear()\n",
    "            aggregated_diff.clear()\n",
    "            summed_vol_diff = 0\n",
    "            init_vol = 0\n",
    "            summed_volume = 0\n",
    "            count = 0\n",
    "            append_flag = True\n",
    "            new_log = True\n",
    "\n",
    "            # Handle gradients for the next log\n",
    "            if cnt_log != logs_N-1:\n",
    "                # Compute and Aggregate Gradients\n",
    "                volume_diff.clear()\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    mass_pred = model(images[remainder:])\n",
    "                    # print(np.shape(mass_pred))\n",
    "                    volume = (mass_pred * speeds[remainder:]) * t\n",
    "                    for ixd, vol in enumerate(volume):\n",
    "                        if ixd == 0:\n",
    "                            summed_vol_diff += tf.squared_difference(volume[ixd], init_vol)\n",
    "                            volume_diff.append(tf.squared_difference(volume[ixd], init_vol))\n",
    "                        else:\n",
    "                            summed_vol_diff += tf.squared_difference(volume[ixd], volume[ixd-1])\n",
    "                            volume_diff.append(tf.squared_difference(volume[ixd], volume[ixd-1]))\n",
    "                    init_vol = volume[Batch_size-remainder-1]\n",
    "\n",
    "                    summed_volume += tf.reduce_sum(volume)\n",
    "                    watched_vars = tape.watched_variables()\n",
    "                grads = tape.gradient(volume, model.trainable_variables)\n",
    "                grads_diff = tape.gradient(volume_diff, model.trainable_variables)\n",
    "                del tape\n",
    "\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    aggregated.append(grad)\n",
    "                    aggregated_diff.append(grads_diff[idx])\n",
    "                append_flag = False\n",
    "    # Validate model every epoch to determine early stopping\n",
    "    MSE_log, MAE_log, MSBE_log, accuracy_, val_step = validate_model(model, val_step, 48, validation_dataset, write_summary=True,\n",
    "                                                                     return_losses=True)\n",
    "    print('\\n')\n",
    "    MSE_t.append(MSE_log)   # contains log losses for each epoch\n",
    "    MAE_t.append(MAE_log)\n",
    "    MSBE_t.append(MSBE_log)\n",
    "    MSE_avg.append(np.mean(MSE_log))    # contains average log losses for each epoch\n",
    "    MAE_avg.append(np.mean(MAE_log))\n",
    "    MSBE_avg.append(np.mean(MSBE_log))\n",
    "    MSE_train.append(loss_metric)\n",
    "    error = 1 - np.mean(accuracy_)\n",
    "    full_data_dict= {\"MSE_t\": MSE_t, \"MAE_t\": MAE_t, \"MSBE_t\": MSBE_t, \"MSE_avg\": MSE_avg, \"MAE_avg\": MAE_avg, \"MSBE_avg\": MSBE_avg, \"MSE_train\": MSE_train}\n",
    "    pickle_out = open(MAIN_dir+\"data_files/\"+\"losses_\"+run_name+\".pickle\", \"wb\")\n",
    "    pk.dump(full_data_dict, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    # save model weights every epoch after 50 epochs lapsed\n",
    "    if performance_flag:\n",
    "        if error <= 0.06:\n",
    "            model.save_weights(checkpont_path.format(log=epoch))  \n",
    "    else:\n",
    "        if epoch >= save_epoch:\n",
    "            model.save_weights(checkpont_path.format(log=epoch))\n",
    "\n",
    "    if error <= 0.02:\n",
    "        epoch = 1000   # end training\n",
    "        break\n",
    "\n",
    "model.save_weights(checkpoint_path + 'cp-'+run_name+'.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_name = 'train_signal.npy'\n",
    "signals = validate_model(model, 1, 143, train_dataset, write_summary=False, return_losses=False)\n",
    "np.save(data_files_path+target_name, signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal = []\n",
    "signalz = []\n",
    "onehot_signals = []\n",
    "volume_signal = []\n",
    "speed_signal = []\n",
    "cnt = 0\n",
    "\n",
    "for sig in tqdm(signals):\n",
    "    for si in sig:\n",
    "        for s in si:\n",
    "            sm = np.float32(np.squeeze(s.numpy()))\n",
    "            signal.append(sm)\n",
    "            volume_signal.append(testing_volume[cnt])\n",
    "            speed_signal.append(testing_speed[cnt])\n",
    "            onehot_signals.append(sm)\n",
    "            cnt += 1\n",
    "    signalz.append([testing_path[cnt-1].split('/')[8], testing_weight[cnt-1], testing_log_length[cnt-1], signal[:len(signal)], \n",
    "                    volume_signal[:len(volume_signal)], speed_signal[:len(speed_signal)]])\n",
    "    signal.clear()\n",
    "    volume_signal.clear()\n",
    "    speed_signal.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = []\n",
    "signalz = []\n",
    "onehot_signals = []\n",
    "volume_signal = []\n",
    "speed_signal = []\n",
    "cnt = 0\n",
    "\n",
    "for sig in tqdm(signals):\n",
    "    for si in sig:\n",
    "        for s in si:\n",
    "            sm = np.float32(np.squeeze(s.numpy()))\n",
    "            signal.append(sm)\n",
    "            volume_signal.append(validation_volume[cnt])\n",
    "            speed_signal.append(validation_speed[cnt])\n",
    "            onehot_signals.append(sm)\n",
    "            cnt += 1\n",
    "    signalz.append([validation_path[cnt-1].split('/')[8], validation_weight[cnt-1], validation_log_length[cnt-1], signal[:len(signal)], \n",
    "                    volume_signal[:len(volume_signal)], speed_signal[:len(speed_signal)]])\n",
    "    signal.clear()\n",
    "    volume_signal.clear()\n",
    "    speed_signal.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = []\n",
    "signalz = []\n",
    "onehot_signals = []\n",
    "volume_signal = []\n",
    "speed_signal = []\n",
    "cnt = 0\n",
    "\n",
    "for sig in tqdm(signals):\n",
    "    for si in sig:\n",
    "        for s in si:\n",
    "            sm = np.float32(np.squeeze(s.numpy()))\n",
    "            signal.append(sm)\n",
    "            volume_signal.append(training_volume[cnt])\n",
    "            speed_signal.append(training_speed[cnt])\n",
    "            onehot_signals.append(sm)\n",
    "            cnt += 1\n",
    "    signalz.append([training_path[cnt-1].split('/')[8], training_weight[cnt-1], training_log_length[cnt-1], signal[:len(signal)], \n",
    "                    volume_signal[:len(volume_signal)], speed_signal[:len(speed_signal)]])\n",
    "    signal.clear()\n",
    "    volume_signal.clear()\n",
    "    speed_signal.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signalz= np.load(MAIN_dir+'Signals/5th_nbrtE_validate.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sig in enumerate(signalz):\n",
    "    prd = np.sum(sig[3])\n",
    "    gt = np.squeeze(sig[1])\n",
    "    mse = np.square(gt-prd)\n",
    "    mae = np.abs(gt-prd)\n",
    "    ACC = (1-mae/gt)*100\n",
    "    ACC = '{:.2f}'.format(ACC)\n",
    "    prd = '{:.2f}'.format(prd)\n",
    "    print('Summary')\n",
    "    print('Log name:', sig[0])\n",
    "    print('Log length:', sig[2])\n",
    "    print('Ground Truth:', gt, 'Prediction:', prd)\n",
    "    print('MSE:', mse, 'MAE:', mae, 'Accuracy:', ACC,'%')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.title('Volume Signal of LOG_X', color='Black')\n",
    "    plt.ylabel('Predicted Volume')\n",
    "    plt.xlabel('Frames(Images)')\n",
    "    # plt.plot(sig[4], 'r', LineWidth=1)\n",
    "    plt.plot(sig[4], '--rs', LineWidth=1, MarkerSize=1, MarkerEdgeColor='k', MarkerFaceColor=[0.5,0.5,0.5])\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# plt.savefig(MAIN_dir+'/Figures/Paper_signals/logx_signal.png', dpi=600)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
